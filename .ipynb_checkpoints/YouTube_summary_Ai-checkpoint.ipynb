{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c289aaa",
   "metadata": {},
   "source": [
    "# 모듈설치\n",
    "1. 유튜브 영상 다운로드 (음성만 다운로드, mp3) - PyTube\n",
    "2. Speech to Text (Transcribe) - OpenAI Whisper (Local)\n",
    "3. Map-reduce summariation - LangChain, OpenAI ChatGPT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec995d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt update && sudo apt install ffmpeg\n",
    "!pip install -q openai-whisper pytube\n",
    "!pip install -q openai tiktoken langchain\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615260a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위스퍼 로드 (음성 -> 텍스트)\n",
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "## 스플릿 로드\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "docs = [Document(page_content=x) for x in text_splitter.split_text(result[\"text\"])]\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "openai_api_key = \"sk-\" # api 키는 유출이 안되게 하라\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Map prompt\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "# Reduce prompt\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes.\n",
    "The final answer is a single paragraph of about 100 words and must be in Korean.\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "\n",
    "# 1. Reduce chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# 2. Map chain\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7853e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import re\n",
    "\n",
    "def extract_video_id(url):\n",
    "    youtube_regex = (r'(https?://)?(www\\.)?'\n",
    "        '(youtube|youtu|youtube-nocookie)\\.(com|be)/'\n",
    "        '(watch\\?v=|embed/|v/|.+\\?v=)?([^&=%\\?]{11})')\n",
    "    youtube_pattern = re.compile(youtube_regex)\n",
    "    match = youtube_pattern.match(url)\n",
    "    if not match:\n",
    "        return None\n",
    "    return match.group(6)\n",
    "\n",
    "def summarize(url):\n",
    "    yt = YouTube(url)\n",
    "\n",
    "    yt.streams.filter(only_audio=True).first().download(\n",
    "        output_path='.', filename='input.mp3')\n",
    "\n",
    "    result = model.transcribe(\"input.mp3\")\n",
    "\n",
    "    docs = [Document(page_content=x) for x in text_splitter.split_text(result[\"text\"])]\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "    sum_result = map_reduce_chain.run(split_docs)\n",
    "\n",
    "    video_id = extract_video_id(url)\n",
    "    embed = f\"\"\"<iframe width='560' height='315' src='https://www.youtube.com/embed/{video_id}' frameborder='0' allowfullscreen></iframe>\"\"\"\n",
    "\n",
    "    return sum_result, embed\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=summarize,\n",
    "    inputs=gr.Textbox(label=\"URL\"),\n",
    "    outputs=[gr.TextArea(label=\"Summary\"), gr.HTML()],\n",
    "    # outputs=gr.TextArea(label=\"Summary\"),\n",
    ")\n",
    "\n",
    "demo.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
